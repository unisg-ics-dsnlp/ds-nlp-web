<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  






























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Blog | DS-NLP Lab</title>

<link rel="icon" href="/preview/pr-12/images/icon.png">

<meta name="title" content="Blog">
<meta name="description" content="Chair of Siegfried Handschuh | Data Science in Natural Language Processing. Chair of Siegfried Handschuh for Data Science in Natural Language Processing at the University of St. Gallen (HSG).">

<meta property="og:title" content="Blog">
<meta property="og:site_title" content="DS-NLP Lab">
<meta property="og:description" content="Chair of Siegfried Handschuh | Data Science in Natural Language Processing. Chair of Siegfried Handschuh for Data Science in Natural Language Processing at the University of St. Gallen (HSG).">
<meta property="og:url" content="/preview/pr-12">
<meta property="og:image" content="/preview/pr-12/images/ICS_HSG-LogoWithWhiteBackground.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Blog">
<meta property="twitter:description" content="Chair of Siegfried Handschuh | Data Science in Natural Language Processing. Chair of Siegfried Handschuh for Data Science in Natural Language Processing at the University of St. Gallen (HSG).">
<meta property="twitter:url" content="/preview/pr-12">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/preview/pr-12/images/ICS_HSG-LogoWithWhiteBackground.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Blog",
    "description": "Chair of Siegfried Handschuh | Data Science in Natural Language Processing. Chair of Siegfried Handschuh for Data Science in Natural Language Processing at the University of St. Gallen (HSG).",
    "headline": "Blog",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/preview/pr-12/images/icon.png" }
    },
    "url": "/preview/pr-12"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/preview/pr-12/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/preview/pr-12/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/all.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/background.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/body.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/button.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/card.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/code.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/details.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/float.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/font.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/form.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/header.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/image.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/link.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/list.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/main.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/section.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/table.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/preview/pr-12/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/preview/pr-12/_scripts/anchors.js"></script>

  <script src="/preview/pr-12/_scripts/dark-mode.js"></script>

  <script src="/preview/pr-12/_scripts/fetch-tags.js"></script>

  <script src="/preview/pr-12/_scripts/search.js"></script>

  <script src="/preview/pr-12/_scripts/site-search.js"></script>

  <script src="/preview/pr-12/_scripts/table-wrap.js"></script>

  <script src="/preview/pr-12/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/preview/pr-12/images/background.jpg')" data-dark="true">
  <a href="/preview/pr-12/" class="home">
    
      <span class="logo">
        
          <?xml version="1.0" encoding="utf-8"?>
<!-- Generator: Adobe Illustrator 26.0.1, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<svg version="1.1" id="Logo" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewbox="0 0 90 100" style="enable-background:new 0 0 90 100;" xml:space="preserve">
<style type="text/css">
	.st0{fill:#00802F;}
</style>
<g>
	<path class="st0" d="M90,70L54.6,57.12c-1.22-0.44-2.15-1.45-2.49-2.71l-0.6-2.19c-0.62-2.27,0.85-4.58,3.16-4.99L90,41V70z
		 M40.96,56.63L38.6,54c-1.26-1.4-3.34-1.73-4.97-0.79L8.1,67.94l4.46,25.28l9.96,0.87L41.46,61.3
		C42.32,59.8,42.12,57.91,40.96,56.63z M35.91,27.5c-0.51-1.48-1.84-2.52-3.4-2.66L0,22l7.05,39.97l32.54-18.79
		c0.83-0.48,1.2-1.48,0.88-2.38c-0.2-0.55-0.63-0.99-1.18-1.19c-2.5-0.9-4.29-3.3-4.29-6.12c0-1.02,0.24-1.99,0.66-2.85
		C36.14,29.67,36.26,28.54,35.91,27.5 M90,0L41.73,22.51c-1.03,0.48-1.5,1.67-1.08,2.73l0.21,0.52c0.29,0.73,0.96,1.2,1.73,1.34
		c2.03,0.35,3.87,1.65,4.81,3.66c1.1,2.36,0.66,5.04-0.91,6.92c-0.38,0.45-0.49,1.07-0.27,1.61c0.32,0.8,1.22,1.21,2.03,0.91L90,25
		V0z M56.12,65.28c-3.85-2.7-9.19-1.51-11.53,2.57L29.18,94.68L90,100V89L56.12,65.28z"></path>
</g>
</svg>

        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">DS-NLP Lab</span>
        
        
          <span class="subtitle">Chair of Siegfried Handschuh | Data Science in Natural Language Processing</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/preview/pr-12/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/preview/pr-12/books/" data-tooltip="Published books">
          Books
        </a>
      
    
      
        <a href="/preview/pr-12/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/preview/pr-12/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/preview/pr-12/blog/" data-tooltip="Musings and miscellany">
          Blog
        </a>
      
    
      
        <a href="/preview/pr-12/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h1 id="blog">
<i class="icon fa-solid fa-feather-pointed"></i>Blog</h1>

<p>On this page you will find an overview of all blog entries in chronological order. The blog posts are intended to provide additional information and insights on current research topics of our research groups.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="tags">
    
      <a href='/preview/pr-12/blog/?search="tag:%20website"' class="tag" data-tooltip='Show items with the tag "website"'>
        website
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20chair"' class="tag" data-tooltip='Show items with the tag "chair"'>
        chair
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20attention-mechanism"' class="tag" data-tooltip='Show items with the tag "attention-mechanism"'>
        attention-mechanism
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20state-space-models"' class="tag" data-tooltip='Show items with the tag "state-space-models"'>
        state-space-models
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20transformers"' class="tag" data-tooltip='Show items with the tag "transformers"'>
        transformers
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20machine-learning"' class="tag" data-tooltip='Show items with the tag "machine-learning"'>
        machine-learning
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20systems-theory"' class="tag" data-tooltip='Show items with the tag "systems-theory"'>
        systems-theory
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20finance"' class="tag" data-tooltip='Show items with the tag "finance"'>
        finance
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20finance-and-ai"' class="tag" data-tooltip='Show items with the tag "finance-and-ai"'>
        finance-and-ai
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20annual-reports"' class="tag" data-tooltip='Show items with the tag "annual-reports"'>
        annual-reports
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20equity-research"' class="tag" data-tooltip='Show items with the tag "equity-research"'>
        equity-research
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20gpt-4"' class="tag" data-tooltip='Show items with the tag "gpt-4"'>
        gpt-4
      </a>
    
      <a href='/preview/pr-12/blog/?search="tag:%20llama-3"' class="tag" data-tooltip='Show items with the tag "llama-3"'>
        llama-3
      </a>
    
  </div>

<div class="search-info"></div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    
      <a href="/preview/pr-12/2025/08/27/SDS2025-2.html" class="post-excerpt-image" aria-label="Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4">
        <img src="/preview/pr-12/images/posts/SDS-2025-2-cover.png" alt="Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4" loading="lazy" onerror="this.src = '/preview/pr-12/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="post-excerpt-text">
      <a href="/preview/pr-12/2025/08/27/SDS2025-2.html">Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4</a>

      <div class="post-info">
  
    
    
      
      
        <span data-tooltip="Author">
          <i class="icon fa-solid fa-feather-pointed"></i>
          <span>jan spörer</span>
        </span>
      
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>August 27, 2025</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>September 03, 2025</span>
    </span>
  
</div>


  


  <div class="tags">
    
      <a href='/preview/pr-12/blog?search="tag:%20finance"' class="tag" data-tooltip='Show items with the tag "finance"'>
        finance
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20finance-and-ai"' class="tag" data-tooltip='Show items with the tag "finance-and-ai"'>
        finance-and-ai
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20annual-reports"' class="tag" data-tooltip='Show items with the tag "annual-reports"'>
        annual-reports
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20equity-research"' class="tag" data-tooltip='Show items with the tag "equity-research"'>
        equity-research
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20machine-learning"' class="tag" data-tooltip='Show items with the tag "machine-learning"'>
        machine-learning
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20gpt-4"' class="tag" data-tooltip='Show items with the tag "gpt-4"'>
        gpt-4
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20llama-3"' class="tag" data-tooltip='Show items with the tag "llama-3"'>
        llama-3
      </a>
    
  </div>





      
      
      <p data-search="Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4Financial equity research reports ERRs are the way banks and independent analyst firms communicate their buy and sell recommendations about individual stocks. Typically, financial experts, so-called analysts, cover a basket of stock-listed companies from one or two industries over a long period of time, and update their ERRs whenever important news come out often after quarterly reports 36 .ERRs typically contain a stock price target as well as a buy, hold, or sell recommendation.The banks clients and the general market use these reports to support investment decisions in the stocks and, to a lesser extent, bonds of these companies.Financial Text One Hard Nut to Crack for AutomationRelevanceIn the financial literature, ERRs are established as important sources of information 44, 45, 46, 47, 48 . They offer some insight into the future stock performance of firms 49 , so they may indeed help investors to make decisions.Issues With Financial Equity Research: Bias and CostBut a problem remains: Many reports are written by banks, and banks have an inherent interest in earning commissions from trades places by their clients. This means they often want to convince potential clients to buy stocks, possibly introducing a reluctance to publish negative coverage 42, 43 . 36 found that only 0.5 of all reports contained sell recommendations before the dot-com bubble burst. Prior research showed less extreme results, with 14 sell recommendations between 1989 and 1991 49 , but the direction is still clear: ERRs seem to be a tad to positive and not entirely unbiased.Another issue with ERRs are cost. As our results below show, much of equity research is just a presentation of financials. Overall, ERRs follow a mechanistic, descriptive pattern. It is thus questionable if significant human input is warranted and useful. Given the concerns around biasedness in equity research, the field may thus benefit from further autmation. Our paper provides the groundwork for systematic automation of equity research.The Structure of Equity ResearchWhile research about the relevance and biasedness of equity research is plentiful, we know very little about what types of statements these reports make. So far, there was no systematic research into the structure of financial equity research reports so that is where our research comes in.Let s start with the basics: Equity research reports are consistently short. Unlike annual reports, which usually have hundreds of pages, equity research is short and concise, and reports with 20 pages are long outliers already.The figures below illustrate that most reports have between 4 and 9 pages, and they do not contain more than 120 statements. Distribution of page counts and statement counts of equity research reports. Most reports have between 4--9 pages and 30--120 statements. Including appendices. Different types of information are systematically presented with different modes of display. Financial information is often structured enough to be displayed in tables and diagrams, while company and market overviews are usually verbal. How Do Banks and Independent Analysts Present Their Analyses We found that text and tables are the dominant ways of conveying information in equity research reports. Diagrams, which often come in the form of stock price charts, are less relevant overall. Custom diagrams that show complex business relationships, product specifications, or market dynamics, are an absolute exception.Textual and tabular statement can often be expressed in various ways, and analysts switch between textual and tabular modes. Different banks tend to have different styles. Some make heavy use of tables, while others present almost all information in text form. This heavy reliance on text and tables is a potential for LLMs, as text and tables are the native domain of language models. Today s language models can write tables in Markdown or in other machine-readable formats. There is significant overlap between presentation modes for the same statements. Textual and tabular representation modes are dominant. Analysts have a strong bias in favor of the type of information that they know best: Financials. This is reflected in the following diagram, which shows that most statements fall into the Financials category. As mentioned above, analysts usually do not take the time to present product-specific diagrams or to display market dynamics or supply chains in graphical form. The category Product illustrates. Statements about the income statement profit amp; loss -- P amp;L are the most frequent question subcategory in our typology. Most of the other top subcategories also fall in the Financials subcategory. An exception is the Stock Price subcategory, which is the second most important subcategory by frequency. How Large is the Dent That LLMs Can Make in Automating Equity Research According to our research, already 55 of the statements from equity research reports are automatable, even without any use of AI LLMs. This is due to the comprehensive data coverage that financial databases such as Bloomberg provide. Executive changes, financials, and subsidiary structures are readily available in structured format in these databases.Thus, to evaluate the automation potential that LLMs can still have, we substracted this 55 from the overall pie, and ended up with 30 of automation potential for LLMs. Thus, only 15 of statements in ERRs cannot be automated.Of these 30 , 26 percentage points were answered correctly by GPT-4, and 27 percentage points were answered correctly by Llama-3. The error rate is reduced when a perfect ensemble of these two models is assumed, as we show below. Language models can close a gap that financial databases leave open. Still, we think that human input is still required, as we assume non-fact based statements opinions and strategic outlooks cannot be automated by LLMs. Potential for Model EnsemblesWe found that GPT-4 and Llama-3 complement each other. In our small-scale analysis, when one of the models fails to answer a question correctly, the other model almost always is able to help out. This result shows that there is potential in multi-AI or multi-agent systems that outperform single-model inference. Ensembles of GPT-4 and Llama-3 have high potential as they show little error overlap. ConclusionThe study shows some patterns in equity research worth pointing out: Analysts usually do not create custom diagrams that deep-dive into product specifications, distribution or supply chains, market structures, or other complex relationships that underly the covered firms business. In fact, only 15 of the statements financial analysts make are hard to automate by LLMs in principle, as these represent strategic outlooks or opinions. Thus, the automation potential of equity research is high, as their human touch is already minuscle today. Most of the statements in financial report relate to financial information -- which can already be found in financial databases and does not require LLMs to be extracted. We still need to caution that analysts perform important roles that LLMs will not be able to replicate. For example, they personally meet management teams and may get subtle clues about the confidence and capabilities of these managers in these meetings. OutlookToday s ERRs heavily focus on financials. This probably arises from the skill set of their authors. Most financial analysts have degrees in business and finance, and these analysts are usually no product or service experts in the industries that they cover. LLMs offer potential for more holistic coverage that takes product and service quality of the covered firms into account, introducing an underappreciated aspect to equity research.Link to the Paper Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4 Adria Pop, Jan Spörer 2025 IEEE Swiss Conference on Data Science SDS nbsp; nbsp; 26 Jun 2025 nbsp; nbsp; doi:10.1109 SDS66131.2025.00025 This research dissects financial equity research reports ERRs by systematically mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4964 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions potential for automation. Answers were labeled text-extractable if the answers to the question were accessible in corporate reports. 75.15 of the questions in ERRs can be automated using text extraction from text sources. Those automatable questions consist of 51.91 text-extractable suited to processing by large language models, LLMs and 24.24 database-extractable questions. Only 24.85 of questions require human judgment to answer. We empirically validate, using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and infor- mation extraction enable the automation of approximately 80 of the statements in ERRs. Surprisingly, the models complement each other s strengths and weaknesses well, indicating strong ensemble potential. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, are available online link removed to preserve anonymity . DOI IEEE financial-ai annual-reports equity-research machine-learning gpt-4 llama-3 References 1 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., et al. 2020 . Language Models Are Few-Shot Learners. Advances in Neural Information Processing Systems NeurIPS . 2 Devlin, J., Chang, M.-W., Lee, K., amp; Toutanova, K. 2019 . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL-HLT . 3 Fawcett, T. 2006 . Introduction to Receiver Operator Curves. Pattern Recognition Letters. 4 Gilbert, E. 2014 . VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. International Conference on Weblogs and Social Media ICWSM . 5 Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., amp; Iwasawa, Y. 2022 . Large Language Models Are Zero-Shot Reasoners. Advances in Neural Information Processing Systems NeurIPS . 6 Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., amp; Neubig, G. 2023 . Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. 7 Maia, M., Handschuh, S., Freitas, A., Davis, B., McDermott, R., Zarrouk, M., amp; Balahur, A. 2018 . WWW 18 Open Challenge: Financial Opinion Mining and Question Answering. Companion Proceedings of the Web Conference. 8 Malo, P., Sinha, A., Korhonen, P., Wallenius, J., amp; Takala, P. 2014 . Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts. Journal of the Association for Information Science and Technology. 9 Liu, Z., Huang, D., Huang, K., Li, Z., amp; Zhao, J. 2021 . FinBERT: A Pre-Trained Financial Language Representation Model for Financial Text Mining. International Conference on Artificial Intelligence. 10 Woodford, M. 2005 . Central Bank Communication and Policy Effectiveness. National Bureau of Economic Research, Cambridge, MA, USA. 11 Hansen, S., McMahon, M., amp; Tong, M. 2019 . The Long-Run Information Effect of Central Bank Communication. Journal of Monetary Economics. 12 Araci, D. 2019 . FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. arXiv. 13 Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., amp; Zettlemoyer, L. 2018 . Deep Contextualized Word Representations. Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL-HLT . 14 Niklaus, C., Freitas, A., amp; Handschuh, S. 2022 . Shallow Discourse Parsing for Open Information Extraction and Text Simplification. Workshop on Computational Approaches to Discourse, International Conference on Computer Linguistics. 15 Chatterjee, N., amp; Agarwal, R. 2023 . Studying the Effect of Syntactic Simplification on Text Summarization. IETE Technical Review. 16 Cetto, M., Niklaus, C., Freitas, A., amp; Handschuh, S. 2018 . Graphene: A Context-Preserving Open Information Extraction System. International Conference on Computational Linguistics: System Demonstrations. 17 Abdel-Nabi, H., Awajan, A., amp; Ali, M. 2023 . Deep Learning-Based Question Answering: A Survey. Knowledge and Information Systems. 18 Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., amp; Grave, E. 2022 . Atlas: Few-Shot Learning with Retrieval Augmented Language Models. arXiv. 19 Guu, K., Lee, K., Tung, Z., Pasupat, P., amp; Chang, M.-W. 2020 . REALM: Retrieval-Augmented Language Model Pre-Training. International Conference on Machine Learning ICML , Proceedings of Machine Learning Research PMLR . 20 Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., amp; Yih, W.-T. 2020 . Dense Passage Retrieval for Open-Domain Question Answering. Conference on Empirical Methods in Natural Language Processing EMNLP . 21 Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-T., Rocktäschel, T., Riedel, S., amp; Kiela, D. 2020 . Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems. 22 Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. 2023 . LLaMA: Open and Efficient Foundation Language Models. arXiv. 23 Press, O., Smith, N., amp; Lewis, M. 2022 . Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. International Conference on Learning Representations ICLR . 24 Antony, D., Abhishek, S., Singh, S., Kodagali, S., Darapaneni, N., Rao, M., Paduri, A. R., amp; Bandalakunta Gururajarao, S. 2023 . A Survey of Advanced Methods for Efficient Text Summarization. IEEE Computing and Communication Workshop and Conference CCWC . 25 Chen, S., Wong, S., Chen, L., amp; Tian, Y. 2023 . Extending Context Window of Large Language Models via Positional Interpolation. arXiv. 26 Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., amp; Mann, G. 2023 . BloombergGPT: A Large Language Model for Finance. arXiv. 27 Fama, E., amp; French, K. 2018 . Choosing Factors. Journal of Financial Economics. 28 Fama, E., amp; French, K. 2015 . A Five-Factor Asset Pricing Model. Journal of Financial Economics. 29 Fama, E., amp; French, K. 1995 . Size and Book-to-Market Factors in Earnings and Returns. The Journal of Finance. 30 Fama, E. 1965 . The Behavior of Stock-Market Prices. The Journal of Business. 31 Fama, E., amp; French, K. 1992 . The Cross-Section of Expected Stock Returns. The Journal of Finance. 32 Carhart, M. 1997 . On Persistence in Mutual Fund Performance. The Journal of Finance. 33 Asness, C., Moskowitz, T., amp; Pedersen, L. 2013 . Value and Momentum Everywhere. The Journal of Finance. 34 Dyer, T., amp; Kim, E. 2021 . Anonymous Equity Research. Journal of Accounting Research. 35 Gleason, C., Johnson, B., amp; Li, H. 2013 . Valuation Model Use and the Price Target Performance of Sell-Side Equity Analysts. Contemporary Accounting Research. 36 Asquith, P., Mikhail, M., amp; Au, A. 2005 . Information Content of Equity Analyst Reports. Journal of Financial Economics. 37 Imam, S., Chan, J., amp; Shah, S. 2013 . Equity Valuation Models and Target Price Accuracy in Europe: Evidence From Equity Reports. International Review of Financial Analysis. 38 Arand, D., Kerl, A., amp; Walter, A. 2015 . When Do Sell-Side Analyst Reports Really Matter Shareholder Protection, Institutional Investors and the Informativeness of Equity Research. European Financial Management. 39 Bonini, S., Zanetti, L., Bianchini, R., amp; Salvi, A. 2010 . Target Price Accuracy in Equity Research. Journal of Business Finance amp; Accounting. 40 Twedt, B., amp; Rees, L. 2012 . Reading Between the Lines: An Empirical Examination of Qualitative Attributes of Financial Analysts Reports. Journal of Accounting and Public Policy. 41 Cheng, W., amp; Ho, J. 2017 . A Corpus Study of Bank Financial Analyst Reports: Semantic Fields and Metaphors. International Journal of Business Communication. 42 Mikhail, M., Walther, B., amp; Willis, R. 2004 . Do Security Analysts Exhibit Persistent Differences in Stock Picking Ability Journal of Financial Economics. 43 Barber, B., Lehavy, R., McNichols, M., amp; Trueman, B. 2001 . Can Investors Profit From the Prophets Security Analyst Recommendations and Stock Returns. The Journal of Finance. 44 Bjerring, J., Lakonishok, J., amp; Vermaelen, T. 1983 . Stock Prices and Financial Analysts Recommendations. The Journal of Finance. 45 Elton, E., Gruber, M., amp; Grossman, S. 1986 . Discrete Expectational Data and Portfolio Performance. The Journal of Finance. 46 Liu, P., Smith, S., amp; Syed, A. 1990 . Stock Price Reactions to the Wall Street Journal s Securities Recommendations. Journal of Financial and Quantitative Analysis. 47 Beneish, M. 1991 . Stock Prices and the Dissemination of Analysts Recommendation. Journal of Business. 48 Stickel, S. 1995 . The Anatomy of the Performance of Buy and Sell Recommendations. Financial Analysts Journal. 49 Michaely, R., amp; Womack, K. 1999 . Conflict of Interest and the Credibility of Underwriter Analyst Recommendations. The Review of Financial Studies. 50 Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., amp; Yvon, F. 2022 . Bloom: A 176b-Parameter Open-Access Multilingual Language Model. arXiv. 51 Rae, J., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., et al. 2022 . Scaling Language Models: Methods, Analysis amp; Insights from Training Gopher. arXiv. 52 Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., amp; Lin, X. V. 2022 . OPT: Open Pre-Trained Transformer Language Models. arXiv. 53 Desislavov, R., Martínez-Plumed, F., amp; Hernández-Orallo, J. 2023 . Trends in AI Inference Energy Consumption: Beyond the Performance-vs-Parameter Laws of Deep Learning. Sustainable Computing: Informatics and Systems. 54 Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.-H., Routledge, B., amp; Wang, W. Y. 2021 . FinQA: A Dataset of Numerical Reasoning over Financial Data. Conference on Empirical Methods in Natural Language Processing EMNLP . 55 Chen, Y., Fu, Q., Yuan, Y., Wen, Z., Fan, G., Liu, D., Zhang, D., Li, Z., amp; Xiao, Y. 2023 . Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models. Proceedings of the 32nd ACM International Conference on Information and Knowledge Management CIKM 23 . 56 Womack, K. 1996 . Do Brokerage Analysts Recommendations Have Investment Value The Journal of Finance. 57 Bradshaw, M., Brown, L., amp; Huang, K. 2013 . Do Sell-Side Analysts Exhibit Differential Target Price Forecasting Ability Review of Accounting Studies. 58 Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., et al. 2021 . Improving Language Models by Retrieving From Trillions of Tokens. International Conference on Machine Learning ICML , Proceedings of Machine Learning Research PMLR . 59 Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., amp; Fung, P. 2023 . Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. 60 Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., amp; Chua, T.-S. 2021 . TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. Annual Meeting of the ACL and International Joint Conference on Natural Language Processing. 61 Coleman, B., Merkley, K., amp; Pacelli, J. 2022 . Human Versus Machine: A Comparison of Robo-Analyst and Traditional Research Analyst Investment Recommendations. The Accounting Review. 62 Malo, P., Sinha, A., Korhonen, P., Wallenius, J., amp; Takala, P. 2014 . Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts. Journal of the Association for Information Science and Technology. 63 Thai, V. T., Davis, B., O Riain, S., O Sullivan, D., amp; Handschuh, S. 2008 . Semantically Enhanced Passage Retrieval for Business Analysis Activity. European Conference on Information Systems ECIS . 64 Kim, A., Muhn, M., amp; Nikolaev, V. 2024 . Financial Statement Analysis with Large Language Models. Chicago Booth Research Paper forthcoming ; Fama-Miller Working Paper. 65 Meta. 2024 . The Llama 3 Herd of Models. Technical Report. 66 OpenAI. 2023 . GPT-4 Technical Report. arXiv. Our Ph.D. candidate Jan Spörer presented the paper.">
        Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4

      </p>
    </div>
  </div>
</div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    
      <a href="/preview/pr-12/2025/08/19/SDS2025.html" class="post-excerpt-image" aria-label="Bridging Attention and State Space Models - A Systems Theory Perspective">
        <img src="/preview/pr-12/images/posts/SDS-SSMAttentionFusion.png" alt="Bridging Attention and State Space Models - A Systems Theory Perspective" loading="lazy" onerror="this.src = '/preview/pr-12/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="post-excerpt-text">
      <a href="/preview/pr-12/2025/08/19/SDS2025.html">Bridging Attention and State Space Models - A Systems Theory Perspective</a>

      <div class="post-info">
  
    
    
      
      
        <span data-tooltip="Author">
          <i class="icon fa-solid fa-feather-pointed"></i>
          <span>götz-henrik wiegand</span>
        </span>
      
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>August 19, 2025</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>September 03, 2025</span>
    </span>
  
</div>


  


  <div class="tags">
    
      <a href='/preview/pr-12/blog?search="tag:%20attention-mechanism"' class="tag" data-tooltip='Show items with the tag "attention-mechanism"'>
        attention-mechanism
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20state-space-models"' class="tag" data-tooltip='Show items with the tag "state-space-models"'>
        state-space-models
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20transformers"' class="tag" data-tooltip='Show items with the tag "transformers"'>
        transformers
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20machine-learning"' class="tag" data-tooltip='Show items with the tag "machine-learning"'>
        machine-learning
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20systems-theory"' class="tag" data-tooltip='Show items with the tag "systems-theory"'>
        systems-theory
      </a>
    
  </div>





      
      
      <p data-search="Bridging Attention and State Space Models: A Systems Theory PerspectiveIn the rapidly evolving landscape of natural language processing, two major paradigms have shaped how we build language models: the Attention Mechanism that powers Transformers 2 , and the recently revived State Space Models SSMs 3,4 . While these approaches seem fundamentally different at first glance, our recent work Integrating the Attention Mechanism Into State Space Models 1 presented at the IEEE Swiss Conference on Data Science reveals surprising connections and proposes a way to combine their strengths. The Tale of Two ArchitecturesAttention Mechanisms: The Context MastersThe attention mechanism, popularized by the Attention is All You Need paper 2 , works by allowing each token in a sequence to look at all previous tokens and decide which ones are most relevant for the current prediction.Think of it like this: when you re reading a sentence and trying to understand what it refers to, you automatically scan back through the text to find the most likely candidate. The attention mechanism does something similar - it computes similarity scores between tokens and uses these to create weighted combinations of past information.Mathematically, for a token at position i, attention computes: y i sum j 1 i a i,j x j W V W O Where a i,j represents how much attention token i pays to token j, computed using query-key similarity: a i,j text softmax x j W K W Q T x i T State Space Models: The Memory KeepersState Space Models take a different approach. Instead of looking back at all previous tokens directly, they maintain a memory state that gets updated as each new token arrives. This state vector acts like a continuously updating summary of everything seen so far.The SSM update equations are elegantly simple: h i h i-1 A x i B quad text update memory y i h i C quad text generate output Here, h i is the memory state, and matrices A , B , C control how information flows through the system.The Hidden Connection: Two Sides of the Same CoinHere s where something truly remarkable emerges from our analysis. Despite looking completely different on the surface, attention mechanisms and state space models are actually solving the same problem in surprisingly similar ways.When we carefully expand the mathematical equations for both approaches, something amazing happens. Both are saying: Take each past token, multiply it by some weight, and add them all up. The profound realization is that both mechanisms are computing weighted averages of past information - they just calculate the weights differently: Attention weights W i,j : How relevant is this past token to what I m trying to understand right now content-based SSM weights V i,j : How much should this past token influence me, given how long ago it was position-based with exponential decay Think of it like this: Attention is like a smart librarian who picks the most relevant books for your research question, regardless of when they were written SSMs are like your memory - recent events are vivid and influential, while older memories fade gradually but systematicallyWhy does this matter This connection reveals that the attention vs. SSM debate isn t about choosing completely different approaches - it s about choosing different strategies for the same fundamental task: deciding how much weight to give to different pieces of past information.Understanding this similarity opens up new possibilities: What if we could combine the best of both worlds What if we could create systems that are both computationally efficient like SSMs AND contextually smart like attention From a signal processing perspective: Attention behaves like a Finite Impulse Response FIR filter: It needs separate parameters for each possible input position SSMs behave like Infinite Impulse Response IIR filters: They use feedback and memory, making them more parameter-efficientThe Trade-off: Context vs EfficiencyThis reveals the fundamental trade-off:Attention s Strength: Context AwarenessAttention excels at capturing which past tokens are contextually relevant, regardless of their position. If John appears 50 tokens back but is crucial for understanding the current sentence, attention can focus on it directly.SSM s Strength: Computational EfficiencySSMs process sequences in linear time and use far fewer parameters. Their memory state provides a compact summary of the entire past sequence, making them ideal for very long sequences where attention s quadratic complexity becomes prohibitive. Recent advances like Mamba 5 have shown how to make SSMs even more efficient with selective state spaces.The LimitationSSMs struggle with explicit context modeling - they can t easily decide that a distant token is more important than a recent one based on semantic similarity. This limitation has been a key motivator for recent work like Mamba 5 and other SSM variants 6,7 .Our Proposed Solution: Context-Aware SSMsWe propose enhancing SSMs with a similarity mechanism inspired by attention. Instead of measuring similarity between current and past tokens like attention does , we measure similarity between the current token and the current memory state: g x i, h i-1 sigma x i W H h i-1 T This similarity score then weights the input: h i h i-1 A x i g x i, h i-1 B y i h i C The IntuitionThink of the memory state h i-1 as containing a compressed representation of all past context. When a new token x i arrives, we check how well it fits with this accumulated context. Tokens that are highly relevant to the current context get stronger weights in the state update.This is like having a conversation where you pay more attention to statements that connect well with the topic you ve been discussing, while still maintaining a continuous thread of memory.Dynamic System PropertiesAn important insight from our analysis concerns the stability properties of SSMs. The eigenvalues of matrix A determine the model s behavior: Stable systems eigenvalues 1 : Information fades gracefully over time Unstable systems eigenvalues gt; 1 : Information grows unboundedly, leading to numerical issues Oscillating systems complex eigenvalues : Create periodic patterns in the outputFor language modeling, we want stable, non-oscillating behavior. This constrains A to have real, positive eigenvalues bounded by 1, which can be achieved using diagonal matrices with sigmoid-activated elements. This insight has been crucial for modern SSM architectures like S4 3 , Mamba 5 , and other recent developments 8,9 .Looking Forward: Implementation ChallengesWhile theoretically elegant, our proposed context-aware SSM extension faces several practical challenges: Training Complexity: The nonlinear similarity term may complicate gradient-based optimization Vanishing Gradients: Like other recurrent models, SSMs can suffer from vanishing gradients over long sequences Computational Overhead: Adding similarity computation increases the computational costHowever, we believe the potential benefits - combining SSMs efficiency with attention s context awareness - make this a promising research direction.ConclusionThe relationship between attention mechanisms and state space models runs deeper than their surface-level differences suggest. Both are solving the same fundamental problem: how to selectively use past information for current predictions.Attention prioritizes semantic relevance, while SSMs prioritize computational efficiency. Our work suggests that we don t have to choose - by integrating attention-like similarity measures into SSMs, we may be able to achieve the best of both worlds.As language models continue to handle longer and longer sequences, finding efficient ways to model context becomes increasingly critical. The marriage of attention and state space concepts may be key to building the next generation of language models that are both computationally efficient and contextually aware.The journey from attention to state space models and back again reminds us that in machine learning, the most powerful solutions often come from understanding and combining different perspectives on the same underlying problem.Link to the Paper Integrating the Attention Mechanism Into State Space Models Tomas Hrycej, Bernhard Bermeitinger, Siegfried Handschuh Zenodo nbsp; nbsp; 27 Jun 2025 nbsp; nbsp; doi:10.5281 zenodo.16380849 Poster from the SDS2025 Conference for the Paper from Hrycej et al. Integrating the Attention Mechanism Into State Space Models . DOI poster qds transformers attention state-space-models References 1 Hrycej, T., Bermeitinger, B., amp; Handschuh, S. 2025 . Integrating the Attention Mechanism into State Space Models. Proceedings of the 2025 IEEE Swiss Conference on Data Science SDS Url: ieeexplore.ieee.org document 11081496. 2 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., amp; Polosukhin, I. 2017 . Attention is all you need. Advances in neural information processing systems, 30, Url: arxiv.org abs 1706.03762 3 Gu, A., Goel, K., amp; Ré, C. 2021 . Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, Url: arxiv.org abs 2111.00396 4 Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., amp; Ré, C. 2021 . Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34, Url: arxiv.org abs 2110.13985 5 Gu, A., amp; Dao, T. 2023 . Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, Url: arxiv.org abs 2312.00752 6 Dao, T., Fu, D., Ermon, S., Rudra, A., amp; Ré, C. 2022 . FlashAttention: Fast and memory-efficient exact attention with IO-awareness. Advances in Neural Information Processing Systems, 35, arxiv.org abs 2205.14135 7 Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M. N., amp; Orvieto, A. 2024 . Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks. arXiv preprint arXiv:2405.15731, Url: arxiv.org abs 2405.15731 8 Smith, J., Warrington, A., amp; Linderman, S. W. 2022 . Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, Url: arxiv.org abs 2208.04933 9 Mehta, H., Gupta, A., Cutkosky, A., amp; Neyshabur, B. 2022 . Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, Url: arxiv.org abs 2206.13947">
        Bridging Attention and State Space Models: A Systems Theory Perspective

      </p>
    </div>
  </div>
</div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    
      <a href="/preview/pr-12/2025/05/23/Hello-World.html" class="post-excerpt-image" aria-label="Welcome to the DS-NLP Lab Website">
        <img src="/preview/pr-12/images/HSG_Daten_01.png" alt="Welcome to the DS-NLP Lab Website" loading="lazy" onerror="this.src = '/preview/pr-12/images/fallback.svg'; this.onerror = null;">
      </a>
    

    <div class="post-excerpt-text">
      <a href="/preview/pr-12/2025/05/23/Hello-World.html">Welcome to the DS-NLP Lab Website!</a>

      <div class="post-info">
  
    
    
      
      
        <span data-tooltip="Author">
          <i class="icon fa-solid fa-feather-pointed"></i>
          <span>götz-henrik wiegand</span>
        </span>
      
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>May 23, 2025</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>September 03, 2025</span>
    </span>
  
</div>


  


  <div class="tags">
    
      <a href='/preview/pr-12/blog?search="tag:%20website"' class="tag" data-tooltip='Show items with the tag "website"'>
        website
      </a>
    
      <a href='/preview/pr-12/blog?search="tag:%20chair"' class="tag" data-tooltip='Show items with the tag "chair"'>
        chair
      </a>
    
  </div>





      
      
      <p data-search="We re excited to welcome you to the official website of our DS-NLP Lab.This platform marks a new chapter for us one where we not only advance research behind the scenes, but also share our journey with the world. We created this website to offer a window into our ongoing work, discoveries, and collaborations. From academic findings to applied insights, this space will keep you up to date with the progress we re making in the different fields we are currently working on.As a research lab and chair, we are committed to pushing the boundaries of what s possible with language and data. But we re equally committed to openness, communication, dialogue and teaching. Whether you re a fellow researcher, a student, an industry partner, or just curious about what we do we re glad you re here.You ll find updates, research highlights, team news, and project showcases.Thanks for visiting and stay tuned">
        We’re excited to welcome you to the official website of our DS-NLP Lab.

      </p>
    </div>
  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->
  </section>


    </main>
    


<footer class="background" style="--image: url('/preview/pr-12/images/background.jpg')" data-dark="true" data-size="wide">
  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://ics.unisg.ch/chairs/siegfried-handschuh-data-science-and-natural-language-processing/" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:siegfried.handschuh@unisg.ch" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-6195-9034" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=zl_3HgQAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


      
    <!-- | &nbsp;
  <img 
    src="/preview/pr-12/images/ICS_HSG-LogoWithWhiteBackground.png" 
    alt="Lab Logo" 
    style="max-height: 42px; width: auto; max-width: 100%; vertical-align: middle;"
  >
  &nbsp;   -->
  
</div>




  <div style="text-align: center;">
    © 2025
    by <a href="https://ics.unisg.ch/chairs/siegfried-handschuh-data-science-and-natural-language-processing/"> 
      Data Science and Natural Language Processing
    </a> at HSG
      |  
    <a href="https://github.com/unisg-ics-dsnlp">
    DS-NLP GitHub
    </a>
     
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
